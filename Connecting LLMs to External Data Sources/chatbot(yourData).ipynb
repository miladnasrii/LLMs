{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV2_rNgvpomb"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit transformers torch langchain chromadb streamlit-chat langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XqfxHQzFBrQk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yahEEKDVEIaP"
      },
      "outputs": [],
      "source": [
        "# CSV dataset path\n",
        "dataset_path = \"Copy your dataset path\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FYFldDM4pffY"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"MBZUAI/LaMini-T5-738M\"\n",
        "persist_directory = \"db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN1X13Hrpr5J"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MKAbBpqBpw3Q"
      },
      "outputs": [],
      "source": [
        "def general_llm_pipeline():\n",
        "    pipe = pipeline(\n",
        "        'text2text-generation',\n",
        "        model=base_model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    general_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    return general_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TEfRLEoppzMJ"
      },
      "outputs": [],
      "source": [
        "def ingest_data():\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    data_texts = data.astype(str).apply(lambda x: \" \".join(x), axis=1).tolist()\n",
        "    documents = [Document(page_content=text) for text in data_texts]\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5V3mgP-6p1DS"
      },
      "outputs": [],
      "source": [
        "def initialize_qa_model(documents):\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(documents, embeddings, persist_directory=persist_directory)\n",
        "    retriever = db.as_retriever()\n",
        "    llm = general_llm_pipeline()\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q0ocEKD4p3ep"
      },
      "outputs": [],
      "source": [
        "def chat_with_bot(user_input, qa_model, general_llm):\n",
        "    if any(keyword in user_input.lower() for keyword in [\"hello\", \"hi\", \"thank\", \"bye\", \"how are you\"]):\n",
        "        return general_llm(user_input)\n",
        "    else:\n",
        "        query = {'query': user_input}\n",
        "        result = qa_model(query)\n",
        "        answer = result['result']\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ruS4IBfIXr3"
      },
      "outputs": [],
      "source": [
        "documents = ingest_data()\n",
        "qa_model = initialize_qa_model(documents)\n",
        "general_llm = general_llm_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFo5beDgIUi6"
      },
      "outputs": [],
      "source": [
        "print(\"Welcome to the Dataset Chatbot! Ask me about the dataset.\")\n",
        "while True:\n",
        "       user_input = input(\"You: \")\n",
        "       if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "           print(\"Goodbye!\")\n",
        "           break\n",
        "       response = chat_with_bot(user_input, qa_model, general_llm)\n",
        "       print(\"Bot:\", response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
