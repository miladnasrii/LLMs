{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szm_At-LJ8H3"
   },
   "outputs": [],
   "source": [
    "!pip install transformers python-docx torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmEftVPHXpV4"
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3j-DX4QX1Ui"
   },
   "outputs": [],
   "source": [
    "document_path = \"your_document_path(CV)\"\n",
    "database_path = \"vector_database.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py4IVp_c_f2C"
   },
   "source": [
    "# Fine-Tune / Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtQjurtC_rUh"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "# dataset = load_dataset(\"squad\")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     eval_dataset=dataset[\"validation\"],\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqNWyM6odGos"
   },
   "outputs": [],
   "source": [
    "def extract_sections_from_docx(doc_path):\n",
    "    doc = Document(doc_path)\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    current_text = []\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.text.isupper():\n",
    "            if current_section:\n",
    "                sections[current_section] = \" \".join(current_text)\n",
    "            current_section = paragraph.text.strip()\n",
    "            current_text = []\n",
    "        else:\n",
    "            current_text.append(paragraph.text.strip())\n",
    "\n",
    "    if current_section:\n",
    "        sections[current_section] = \" \".join(current_text)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQi8Ea_EdHlx"
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7wmxAHDdMNU"
   },
   "outputs": [],
   "source": [
    "def save_vectors_to_database(sections, database_path):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS vectors\")\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS vectors (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        section TEXT,\n",
    "        vector TEXT,\n",
    "        text TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    for section, text in sections.items():\n",
    "        vector = text_to_vector(text)\n",
    "        vector_str = \",\".join(map(str, vector))\n",
    "        cursor.execute(\"INSERT INTO vectors (section, vector) VALUES (?, ?)\", (section, vector_str))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ImdVGCWgVwt"
   },
   "outputs": [],
   "source": [
    "def find_exact_answer(query, database_path):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT section, vector, text FROM vectors\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    vectors = []\n",
    "    sections = []\n",
    "    texts = []\n",
    "    for row in rows:\n",
    "        sections.append(row[0])\n",
    "        vectors.append(np.array(list(map(float, row[1].split(',')))))\n",
    "        texts.append(row[2] if row[2] else \"\")\n",
    "\n",
    "    query_vector = text_to_vector(query)\n",
    "\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    similarities = [cosine_similarity(query_vector, v) for v in vectors]\n",
    "    max_index = np.argmax(similarities)\n",
    "\n",
    "    related_section_text = texts[max_index]\n",
    "\n",
    "    if not related_section_text.strip():\n",
    "        return sections[max_index], \"No text available in this section.\", 0.0\n",
    "\n",
    "    MAX_CONTEXT_LENGTH = 512\n",
    "    related_section_text = related_section_text[:MAX_CONTEXT_LENGTH]\n",
    "\n",
    "    MAX_SENTENCES = 5\n",
    "    sentences = sent_tokenize(related_section_text)\n",
    "    related_section_text = \" \".join(sentences[:MAX_SENTENCES])\n",
    "\n",
    "    if not query.strip():\n",
    "        raise ValueError(\"The query/question cannot be empty.\")\n",
    "\n",
    "    qa_model = pipeline(\"question-answering\", model=\"deepset/roberta-large-squad2\")\n",
    "    answer = qa_model(question=query, context=related_section_text)\n",
    "\n",
    "    if not answer or 'answer' not in answer:\n",
    "        return sections[max_index], \"No valid answer found.\", 0.0\n",
    "\n",
    "    return sections[max_index], answer['answer'], answer['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoiQKGagdTc5"
   },
   "outputs": [],
   "source": [
    "sections = extract_sections_from_docx(document_path)\n",
    "save_vectors_to_database(sections, database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aacxHcgndVoh"
   },
   "outputs": [],
   "source": [
    "query = \"What is his email address?\"\n",
    "section, exact_answer, confidence = find_exact_answer(query, database_path)\n",
    "print(f\"Closest section: {section}\")\n",
    "print(f\"Answer: {exact_answer} (Confidence: {confidence:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
